{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ecefaf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    <h1> Tutorial 1 </h1> \n",
    "    <h2> Classification of Volcanic Deformation using Convolutional Neural Networks </h2>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2835391",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "Load in all required modules (including some auxiliary code) and turn off warnings. Make sure Keras session is clear\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability: disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b2c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 21:43:00.244467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 21:43:00.244488: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/helen/WORK/LIFD_ConvolutionalNeuralNetworks/.github/workflows/../../aux_functions.py:377: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if flip is 'up_down':                                       # conver the string input to a value\n",
      "/home/helen/WORK/LIFD_ConvolutionalNeuralNetworks/.github/workflows/../../aux_functions.py:380: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif flip is 'left_right':\n",
      "/home/helen/WORK/LIFD_ConvolutionalNeuralNetworks/.github/workflows/../../aux_functions.py:383: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif flip is 'both':\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "# general file system utilites\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Maths and \n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "# Premade data is provided as pickles\n",
    "import pickle\n",
    "# Plotting utilies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "# Machine learning Library Keras\n",
    "from tensorflow import keras \n",
    "#from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "# import axillary plotting functions\n",
    "# these functions \n",
    "sys.path.append('../../')\n",
    "from aux_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c857c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Keras session\n",
    "K.clear_session()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6eb0c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "\n",
    "\n",
    "# Load In Provided Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70fdf2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "    \n",
    "The model will be trained on a large dataset of synthetic interferograms which feature labels of both the type and location of any deformation. The performance improved by including a small amount of augmented real Sentinel-1 data.\n",
    "\n",
    "The first steps to preparing the data to have to be pre-made synthetic interferograms are provided in the `data` folder.\n",
    "\n",
    "<h1> Sythetic Interferograms </h1>\n",
    "\n",
    "As this is a tutorial focusing on Machine learning the Synthetic Interferograms are provided as pickle files. These files were genrated using [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy). [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy) generates synthetic images similar to those produced by Sentinel-1 satellites from the SRTM3 digital elevation model (DEM) <sup>[2]</sup>\n",
    "   \n",
    "[2] [Gaddes & Bagnardi 2019](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019JB017519)\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf960b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffcdcc; padding: 10px;\">    \n",
    "\n",
    "\n",
    "If you wanted to generate your own synthetic data you would need to use the tools in [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy). [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy) and the pickled volcano dem data `data/volcano_dems.pkl`\n",
    "\n",
    "\n",
    "    \n",
    "```bash\n",
    " git submodule add https://github.com/matthew-gaddes/SyInterferoPy SyInterferoPy\n",
    " dependency_paths = {'syinterferopy_bin' : 'SyInterferoPy/lib/}\n",
    " sys.path.append(dependency_paths['syinterferopy_bin'])\n",
    " \n",
    "```    \n",
    "```python\n",
    "from random_generation_functions import create_random_synthetic_ifgs  \n",
    "os.mkdir(Path(f\"./data/synthetic_data/\"))\n",
    "for file_n in range(synthetic_ifgs_n_files):\n",
    "    print(f\"Generating file {file_n} of {synthetic_ifgs_n_files} files.  \")\n",
    "    X_all, Y_class, Y_loc, Y_source_kwargs = create_random_synthetic_ifgs(volcano_dems, \n",
    "                                                                **synthetic_ifgs_settings)\n",
    "    # convert to one hot encoding (from class labels)\n",
    "    Y_class = keras.utils.to_categorical(Y_class, len(synthetic_ifgs_settings['defo_sources']), \n",
    "                                                        dtype='float32')          \n",
    "    with open(Path(f'./data/synthetic_data/data_file_{file_n}.pkl'), 'wb') as f:\n",
    "        pickle.dump(X_all[synthetic_ifgs_settings['outputs'][0]], f)                                               \n",
    "        pickle.dump(Y_class, f)\n",
    "        pickle.dump(Y_loc, f)\n",
    "    f.close()\n",
    "    del X_all, Y_class, Y_loc\n",
    "# output the settings as a text file so that we know how data were generated in the future.  \n",
    "with open(f\"./data/synthetic_data/synth_data_settings.txt\", 'w') as f:       \n",
    "    print(f\"Number of data per file : {ifg_settings['n_per_file']}\" ,file = f)\n",
    "    print(f\"Number of files: {synthetic_ifgs_n_files}\" ,file = f)\n",
    "    for key in synthetic_ifgs_settings:\n",
    "        print(f\"{key} : {synthetic_ifgs_settings[key]}\", file = f)\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cd53f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "    <h3>Settings for generating interfeorgrams.</h3>\n",
    "\n",
    "Passing this information to [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy). [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy) will generate 650 synthetic Interfograms\n",
    "\n",
    "* **n_per_file:** number of ifgs per data file.  \n",
    "* **synthetic_ifgs_n_files:** numer of files of synthetic data\n",
    "* **defo_sources:** deformation patterns that will be included in the dataset.  \n",
    "* **n_ifgs:** the number of synthetic interferograms to generate PER FILE\n",
    "* **n_pix:** number of 3 arc second pixels (~90m) in x and y direction\n",
    "* **outputs:**  channel outputs.  uuu = unwrapped across all 3\n",
    "* **intermediate_figure:**  if True, a figure showing the steps taken during creation of each ifg is displayed.  \n",
    "* **cov_coh_scale:** The length scale of the incoherent areas, in meters.  A smaller value creates smaller patches, and a larger one creates larger pathces.  \n",
    "* **coh_threshold:** if 1, there are no areas of incoherence, if 0 all of ifg is incoherent.  \n",
    "* **min_deformation:** deformation pattern must have a signals of at least this many metres.  \n",
    "* **max_deformation:** deformation pattern must have a signal no bigger than this many metres.  \n",
    "* **snr_threshold signal:** to noise ratio (deformation vs turbulent and topo APS) to ensure that deformation is visible.  A lower value creates more subtle deformation signals.\n",
    "* **turb_aps_mean:** turbulent APS will have, on average, a maximum strenghto this in metres (e.g 0.02 = 2cm)\n",
    "* **turb_aps_length:** turbulent APS will be correlated on this length scale, in metres.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d523fc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining if files containing the synthetic deformation patterns exist...\n",
      " \n",
      "The correct number of files were found (13) so no new ones will be generated.  \n",
      "However, this doesn't guarantee that the files were made using the settings in synthetic_ifgs_settings.\n",
      "Check synth_data_settings.txt to be sure.   \n"
     ]
    }
   ],
   "source": [
    "# Define some settings (outlined above)\n",
    "ifg_settings            = {'n_per_file'         : 50}    # number of ifgs per data file.  \n",
    "synthetic_ifgs_n_files  =  13                            # numer of files of synthetic data\n",
    "synthetic_ifgs_settings = {'defo_sources'       : ['dyke', 'sill', 'no_def'],  \n",
    "                           'n_ifgs'             : ifg_settings['n_per_file'],  \n",
    "                           'n_pix'              : 224,   \n",
    "                           'outputs'            : ['uuu'],\n",
    "                           'cov_coh_scale'      : 5000,  \n",
    "                           'coh_threshold'      : 0.7,  \n",
    "                           'min_deformation'    : 0.05,  \n",
    "                           'max_deformation'    : 0.25,\n",
    "                           'snr_threshold'      : 2.0, \n",
    "                           'turb_aps_mean'      : 0.02, \n",
    "                           'turb_aps_length'    : 5000} \n",
    "n_synth_data = ifg_settings['n_per_file'] * synthetic_ifgs_n_files\n",
    "print('\\nDetermining if files containing the synthetic deformation patterns exist...\\n ', end = '')\n",
    "synthetic_data_files = glob.glob(str(Path(f\"../../data/synthetic_data/*.pkl\")))                 \n",
    "if len(synthetic_data_files) == synthetic_ifgs_n_files:\n",
    "    print(f\"\\nThe correct number of files were found ({synthetic_ifgs_n_files}) so no new ones will be generated.  \"\n",
    "          f\"\\nHowever, this doesn't guarantee that the files were made using the settings in synthetic_ifgs_settings.\" \n",
    "          f\"\\nCheck synth_data_settings.txt to be sure.   \")\n",
    "else:\n",
    "    print(f\"\\nCheck for pickles- do you have the data files required (in GitHub Repo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e22512",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Load in Real Data</h1>\n",
    "    \n",
    "Included in this repository is a git submodule [VolcNet](https://github.com/matthew-gaddes/VolcNet) which is a set of 250 labelled unwrapped interferograms that contain labels of both the type of deformation (including examples of no deformation) and the location of deformation within the interferograms. In the form of pickle files, i.e. interferograms have been stored as masked NumPy arrays and labelled with location and deformation source.\n",
    "    \n",
    "If you have not already then in your repository directory please run the following code. \n",
    "    \n",
    "```bash\n",
    "git submodule init\n",
    "git submodule update --init --recursive\n",
    "```\n",
    "\n",
    "This labelled data will be used to train our model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72fa7b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "    \n",
    "The below code checks for the Volcnet files and uses plotting functions in the provided `aux_functions.py` to show the data.\n",
    "    \n",
    "For an example file, it will show a plot of the interferogram and give the label of the deformation source (if applicable).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1db55e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No VolcNet files have been found.Perhaps the path is wrong? Or perhaps you only want to use synthetic data?In which case, this section can be removed.  Exiting...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_126153/407063050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mVolcNet_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVolcNet_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'*.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVolcNet_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     raise Exception('No VolcNet files have been found.'  +\n\u001b[0m\u001b[1;32m     11\u001b[0m                     \u001b[0;34m'Perhaps the path is wrong? Or perhaps you only want to use synthetic data?'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     'In which case, this section can be removed.  Exiting...')\n",
      "\u001b[0;31mException\u001b[0m: No VolcNet files have been found.Perhaps the path is wrong? Or perhaps you only want to use synthetic data?In which case, this section can be removed.  Exiting..."
     ]
    }
   ],
   "source": [
    "# Load the real data\n",
    "# Note that these are in metres, and use one hot encoding for the class, \n",
    "# and are masked arrays (incoherence and water are masked)\n",
    "VolcNet_path = Path('../../VolcNet')\n",
    "# factor to auument by.  E.g. if set to 3 and there are 250 data, there will be 650 augmented   \n",
    "real_ifg_settings       = {'augmentation_factor' : 3}\n",
    "#  get a list of the paths to all the VolcNet files\n",
    "VolcNet_files = sorted(glob.glob(str(VolcNet_path / '*.pkl')))         \n",
    "if len(VolcNet_files) == 0:\n",
    "    raise Exception('No VolcNet files have been found.'  +\n",
    "                    'Perhaps the path is wrong? Or perhaps you only want to use synthetic data?'+  \n",
    "                    'In which case, this section can be removed.  Exiting...')\n",
    "\n",
    "X_1s = []\n",
    "Y_class_1s = []\n",
    "Y_loc_1s = []\n",
    "for VolcNet_file in VolcNet_files:\n",
    "    X_1, Y_class_1, Y_loc_1 = open_VolcNet_file(VolcNet_file, synthetic_ifgs_settings['defo_sources'])\n",
    "    X_1s.append(X_1)\n",
    "    Y_class_1s.append(Y_class_1)\n",
    "    Y_loc_1s.append(Y_loc_1)\n",
    "X = ma.concatenate(X_1s, axis = 0)\n",
    "Y_class = np.concatenate(Y_class_1s, axis = 0)\n",
    "Y_loc = np.concatenate(Y_loc_1s, axis = 0)\n",
    "del X_1s, Y_class_1s, Y_loc_1s, X_1, Y_class_1, Y_loc_1\n",
    "# plot the data in it (note that this can be across multiople windows) \n",
    "plot_data_class_loc_caller(X[:30,], Y_class[:30,], Y_loc[:30,], source_names = ['dyke', 'sill', 'no def'], window_title = 'Sample of Real data')               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78557fb9",
   "metadata": {},
   "source": [
    "# Augment Real Data\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "To improve the performance of the model real data is incorporated. Because we can't include as much real data as the synthetic data we must 'augment' the data into the same size as the number of synthetic interferograms by creating random flips,  rotations,  and translations\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095dd17",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "    \n",
    "As this augmentation is purely data manipulation we'll use some functions from `aux_functions.py` to help augment the set of 250 interferograms and generate a set of 650 augmented interferograms. The below code will generate pickle files of the augmented data that you can reuse so this step only needs to be done once.\n",
    "    \n",
    "<br>    \n",
    "    \n",
    "If you've already done this the below code will check if the pickles already exist and only calculated the augmented data if required\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c1174c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_126153/3527326264.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_augmented_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal_ifg_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'augmentation_factor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mifg_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_per_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# detemine how many files will be needed, given the agumentation factor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Determining if files containing the augmented real data exist.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreal_augmented_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./data/real/augmented/*.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_augmented_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_augmented_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     print(f\"    The correct number of augmented real data files were found ({n_augmented_files}) \"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "n_augmented_files = int((X.shape[0] * real_ifg_settings['augmentation_factor']) / ifg_settings['n_per_file'])                   # detemine how many files will be needed, given the agumentation factor.  \n",
    "print('    Determining if files containing the augmented real data exist.')\n",
    "real_augmented_files = glob.glob(str(Path(f\"./data/real/augmented/*.pkl\")))             #\n",
    "if len(real_augmented_files) == n_augmented_files:\n",
    "    print(f\"    The correct number of augmented real data files were found ({n_augmented_files}) \"\n",
    "          f\"so no new ones will be generated.  \"\n",
    "          f\"However, this doesn't guarantee that the files were made using the current real data.  \")\n",
    "else:\n",
    "        try:\n",
    "            shutil.rmtree(str(Path(f\"./data/real/augmented/\")))\n",
    "        except:\n",
    "            pass\n",
    "        os.mkdir((Path(f\"./data/real/\")))\n",
    "        os.mkdir((Path(f\"./data/real/augmented/\")))\n",
    "        print(f\"There are {X.shape[0]} real data and the augmentation factor is set\" +\n",
    "              f\"to {real_ifg_settings['augmentation_factor']}.  \")\n",
    "        print(f\"    With {ifg_settings['n_per_file']} data per file, the nearest integer\" + \n",
    "              f\"number of files is {n_augmented_files}.  \")\n",
    "        # loop through each file that is to be made\n",
    "        for n_augmented_file in range(n_augmented_files):                                                                               \n",
    "            print(f'    File {n_augmented_file} of {n_augmented_files}...', end = '')  \n",
    "            X_sample, Y_class_sample, Y_loc_sample = choose_for_augmentation(X, Y_class, Y_loc,                                         # make a new selection of the data with balanced classes\n",
    "                                                                              n_per_class = int(X.shape[0] / Y_class.shape[1]))          # set it so there are as many per class as there are (on average) for the real data.  \n",
    "            X_aug, Y_class_aug, Y_loc_aug = augment_data(X_sample, Y_class_sample, Y_loc_sample,                                        # augment the sample of real data\n",
    "                                                          n_data = ifg_settings['n_per_file'])                                           # make as many new data as are set to be in a single file.  \n",
    "        \n",
    "            with open(f\"./data/real/augmented/data_file_{n_augmented_file}.pkl\", 'wb') as f:                                        # save the output as a pickle\n",
    "                pickle.dump(X_aug, f)\n",
    "                pickle.dump(Y_class_aug, f)\n",
    "                pickle.dump(Y_loc_aug, f)\n",
    "            f.close()\n",
    "            print('Done!')\n",
    "        # fill variable with new generated files\n",
    "        real_augmented_files = glob.glob(str(Path(f\"./data/real/augmented/*.pkl\")))  \n",
    "        print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f50e9f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Merging real and synthetic inteferograms and rescaling to CCN's input range</h1>\n",
    "\n",
    "First, we're going to merge our two datasets and format them into an output range suitable for the CNN used. E.g. our data might be in meters and rads and we need to rescale to values in the RGB range 0-255 (python's first index is 0 so 0 -255 gives 256 values)\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9220ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_settings = {'input_range': {'min':0, 'max':255}}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def merge_and_rescale_data(synthetic_data_files, real_data_files, output_range = {'min':0, 'max':225}):\n",
    "    \"\"\" Given a list of synthetic data files and real data files (usually the augmented real data),\n",
    "    \n",
    "    Inputs:\n",
    "        synthetic_data_files | list of Paths or string | locations of the .pkl files containing the masked arrays\n",
    "        reak_data_files      | list of Paths or string | locations of the .pkl files containing the masked arrays\n",
    "        output_range         | dict                    | min and maximum of each channel in each image. \n",
    "                                                         Should be set to suit the CNN being used.  \n",
    "    Returns:\n",
    "        .npz files in step_04_merged_rescaled_data\n",
    "    History:\n",
    "        2020_10_29 | MEG | Written\n",
    "        2021_01_06 | MEG | Fix bug in that mixed but not rescaled data was being written to the numpy arrays.  \n",
    "    \"\"\"  \n",
    "    def data_channel_checker(X, n_cols = None, window_title = None):\n",
    "        \"\"\" Plot some of the data in X.   All three channels are shown.  \n",
    "        \"\"\"      \n",
    "        if n_cols == None:       # if n_cols is None, we'll plot all the data\n",
    "            n_cols = X.shape[0]   # so n_cols is the number of data\n",
    "            plot_args = np.arange(0, n_cols) # and we'll be plotting each of them\n",
    "        else:\n",
    "            plot_args = np.random.randint(0, X.shape[0], n_cols)        # else, pick some at random to plot\n",
    "        f, axes = plt.subplots(3,n_cols)\n",
    "        if window_title is not None:\n",
    "            f.canvas.set_window_title(window_title)\n",
    "        for plot_n, im_n in enumerate(plot_args):                           # loop through each data (column)               \n",
    "            axes[0, plot_n].set_title(f\"Data: {im_n}\")\n",
    "            for channel_n in range(3):                                      # loop through each row\n",
    "                axes[channel_n, plot_n].imshow(X[im_n, :,:,channel_n])\n",
    "                if plot_n == 0:\n",
    "                    axes[channel_n, plot_n].set_ylabel(f\"Channel {channel_n}\")\n",
    "\n",
    "    if len(synthetic_data_files) != len(real_data_files):\n",
    "        raise Exception('This funtion is only designed to be used when the number of real and synthetic data files are the same.  Exiting.  ')\n",
    "\n",
    "    n_files = len(synthetic_data_files)        \n",
    "    out_file = 0\n",
    "    try:\n",
    "            shutil.rmtree(str(Path(f\"./data/merged_out/\")))\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir((Path(f\"./data/merged_out\")))# \n",
    "    for n_file in range(n_files):\n",
    "        print(f'    Opening and merging file {n_file} of each type... ', end = '')\n",
    "        with open(real_data_files[n_file], 'rb') as f:       # open the real data file\n",
    "            X_real = pickle.load(f)\n",
    "            Y_class_real = pickle.load(f)\n",
    "            Y_loc_real = pickle.load(f)\n",
    "        f.close()    \n",
    "        \n",
    "        with open(synthetic_data_files[n_file], 'rb') as f:       # open the synthetic data file\n",
    "            X_synth = pickle.load(f)\n",
    "            Y_class_synth = pickle.load(f)\n",
    "            Y_loc_synth = pickle.load(f)\n",
    "        f.close()    \n",
    "\n",
    "        X = ma.concatenate((X_real, X_synth), axis = 0)        # concatenate the data\n",
    "        Y_class = ma.concatenate((Y_class_real, Y_class_synth), axis = 0)    # and the class labels\n",
    "        Y_loc = ma.concatenate((Y_loc_real, Y_loc_synth), axis = 0)        # and the location labels\n",
    "        \n",
    "        mix_index = np.arange(0, X.shape[0])          # mix them, get a lis of arguments for each data \n",
    "        np.random.shuffle(mix_index)            # shuffle the arguments\n",
    "        X = X[mix_index,]               # reorder the data using the shuffled arguments\n",
    "        Y_class = Y_class[mix_index]     # reorder the class labels\n",
    "        Y_loc = Y_loc[mix_index]      # and the location labels\n",
    "        # resacle the data from metres/rads etc. to desired input range of cnn (e.g. [0, 255]), \n",
    "        # and convert to numpy array\n",
    "        X_rescale = custom_range_for_CNN(X, output_range, mean_centre = False)                  \n",
    "        data_mid = int(X_rescale.shape[0] / 2)\n",
    "        np.savez(f'data/merged_out/data_file_{out_file}.npz', \n",
    "                 X = X_rescale[:data_mid,:,:,:], \n",
    "                 Y_class= Y_class[:data_mid,:], \n",
    "                 Y_loc = Y_loc[:data_mid,:])           # save the first half of the data\n",
    "        out_file += 1                                                                                                                                                   # after saving once, update\n",
    "        np.savez(f'data/merged_out/data_file_{out_file}.npz', \n",
    "                 X = X_rescale[data_mid:,:,:,:], \n",
    "                 Y_class= Y_class[data_mid:,:],\n",
    "                 Y_loc = Y_loc[data_mid:,:])           # save the second half of the data\n",
    "        out_file += 1                                                                                                                                                   # and after saving again, update again.  \n",
    "        print('Done.  ')\n",
    "        \n",
    "def expand_to_r4(r2_array, shape = (224,224)):\n",
    "    \"\"\"\n",
    "    Calcaulte something for every image and channel in rank 4 data (e.g. 100x224x224x3 to get 100x3)\n",
    "    Expand new rank 2 to size of original rank 4 for elemtiwise operations\n",
    "    \"\"\"\n",
    "    \n",
    "    r4_array = r2_array[:, np.newaxis, np.newaxis, :]\n",
    "    r4_array = np.repeat(r4_array, shape[0], axis = 1)\n",
    "    r4_array = np.repeat(r4_array, shape[1], axis = 2)\n",
    "    return r4_array\n",
    "\n",
    "def custom_range_for_CNN(r4_array, min_max, mean_centre = False):\n",
    "    \"\"\" Rescale a rank 4 array so that each channel's image lies in custom range\n",
    "    e.g. input with range of (-5, 15) is rescaled to (-125 125) or (-1 1) for use with VGG16.  \n",
    "    Designed for use with masked arrays.  \n",
    "    Inputs:\n",
    "        r4_array | r4 masked array | works with masked arrays?  \n",
    "        min_max | dict | 'min' and 'max' of range desired as a dictionary.  \n",
    "        mean_centre | boolean | if True, each image's channels are mean centered.  \n",
    "    Returns:\n",
    "        r4_array | rank 4 numpy array | masked items are set to zero, rescaled so that each channel for each image lies between min_max limits.  \n",
    "    History:\n",
    "        2019/03/20 | now includes mean centering so doesn't stretch data to custom range.  \n",
    "                    Instead only stretches until either min or max touches, whilst mean is kept at 0\n",
    "        2020/11/02 | MEG | Update so range can have a min and max, and not just a range\n",
    "        2021/01/06 | MEG | Upate to work with masked arrays.  Not test with normal arrays.\n",
    "    \"\"\"\n",
    "    if mean_centre:\n",
    "        # get the average for each image (in all thre channels)\n",
    "        im_channel_means = ma.mean(r4_array, axis = (1,2)) \n",
    "        # expand to r4 so we can do elementwise manipulation\n",
    "        im_channel_means = expand_to_r4(im_channel_means, r4_array[0,:,:,0].shape)          \n",
    "        # do mean centering    \n",
    "        r4_array -= im_channel_means                                                                        \n",
    "\n",
    "    # get the minimum of each image and each of its channels    \n",
    "    im_channel_min = ma.min(r4_array, axis = (1,2))\n",
    "    # exapnd to rank 4 for elementwise applications\n",
    "    im_channel_min = expand_to_r4(im_channel_min, r4_array[0,:,:,0].shape)\n",
    "    # set so lowest channel for each image is 0\n",
    "    r4_array -= im_channel_min                                                              \n",
    "    # get the maximum of each image and each of its channels\n",
    "    im_channel_max = ma.max(r4_array, axis = (1,2)) \n",
    "    # make suitable for elementwise applications\n",
    "    im_channel_max = expand_to_r4(im_channel_max, r4_array[0,:,:,0].shape) \n",
    "    r4_array /= im_channel_max  # should now be in range [0, 1]\n",
    "    \n",
    "    r4_array *= (min_max['max'] - min_max['min'])    # should now be in range [0, new max-min]\n",
    "    r4_array += min_max['min']                # and now in range [new min, new max]\n",
    "    # convert to numpy array, maksed incoherent areas are set to zero.  \n",
    "    r4_nparray = r4_array.filled(fill_value = 0)         \n",
    "    return r4_nparray \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7eabcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "UsePreMadeBottlenecks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6a1196",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 21:43:48.511743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-30 21:43:48.511775: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-30 21:43:48.511806: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (marvin): /proc/driver/nvidia/version does not exist\n",
      "2022-06-30 21:43:48.512012: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using premade bottleneck files\n"
     ]
    }
   ],
   "source": [
    "# Compute bottleneck features):\n",
    "\n",
    "# load the first 5 (convolutional) blocks of VGG16 and their weights.\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))     \n",
    "\n",
    "  \n",
    "data_out_files = sorted(glob.glob(f'./data/merged_out/*.npz'))       \n",
    "\n",
    "if UsePreMadeBottlenecks is False:\n",
    "    try:\n",
    "            shutil.rmtree(str(Path(f\"./data/bottleneck_out/\")))\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir((Path(f\"./data/bottleneck_out\")))# \n",
    "    bottleneck_folder = 'bottleneck_out'\n",
    "    # get a list of the files output by step 05 (augmented real data and synthetic data mixed and \n",
    "    # rescaed to correct range, with 0s for masked areas.  )\n",
    "    for file_n, data_out_file in enumerate(data_out_files):   \n",
    "        # loop through each of the step 05 files.  \n",
    "        print(f'Bottlneck file {file_n}:') \n",
    "        data_out_file = Path(data_out_file)                                                                                     # convert to path \n",
    "        bottleneck_file_name = data_out_file.parts[-1].split('.')[0]                                                            # and get last part which is filename    \n",
    "        data = np.load(data_out_file)                                                                                           # load the numpy file\n",
    "        X = data['X']                                                                                                           # extract the data for it\n",
    "        Y_class = data['Y_class']                                                                                               # and class labels.  \n",
    "        Y_loc = data['Y_loc']                                                                                                   # and location labels.  \n",
    "        X_btln = vgg16_block_1to5.predict(X, verbose = 1)      \n",
    "        # predict up to bottleneck    \n",
    "        np.savez(f'data/{bottleneck_folder}/{bottleneck_file_name}_bottleneck.npz', \n",
    "                 X = X_btln, Y_class = Y_class, Y_loc = Y_loc)   \n",
    "        # save the bottleneck file, and the two types of label.\n",
    "else:\n",
    "    print('using premade bottleneck files')\n",
    "    bottleneck_folder = 'bottleneck_provided'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60a69c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Training the Neural Network </h1>\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014619c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    " First, we need two functions to divide the list into a training and testing dataset. Two functions are written below to divide the data files and bottleneck files into testing and training datasets and to load various features into arrays in your computers RAM\n",
    "\n",
    "```python\n",
    "    \n",
    "\n",
    "train_test_validate = file_list_divider(data_files, cnn_settings['n_files_train'], \n",
    "                                         cnn_settings['n_files_validate'], \n",
    "                                         cnn_settings['n_files_test']) \n",
    "# assign the outputs    \n",
    "[data_files_train, data_files_validate, data_files_test] = train_test_validate \n",
    "```\n",
    "    \n",
    "creates a set of files to use for training, validation and testing based on prescribed CNN settings\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51058f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_list_divider(file_list, n_files_train, n_files_validate, n_files_test):\n",
    "    \"\"\" Given a list of files, divide it up into training, validating, and testing lists.  \n",
    "    Inputs\n",
    "        file_list | list | list of files\n",
    "        n_files_train | int | Number of files to be used for training\n",
    "        n_files_validate | int | Number of files to be used for validation (during training)\n",
    "        n_files_test | int | Number of files to be used for testing\n",
    "    Returns:\n",
    "        file_list_train | list | list of training files\n",
    "        file_list_validate | list | list of validation files\n",
    "        file_list_test | list | list of testing files\n",
    "    History:\n",
    "        2019/??/?? | MEG | Written\n",
    "        2020/11/02 | MEG | Write docs\n",
    "        \"\"\"\n",
    "    file_list_train = file_list[:n_files_train]\n",
    "    file_list_validate = file_list[n_files_train:(n_files_train+n_files_validate)]\n",
    "    file_list_test = file_list[(n_files_train+n_files_validate) : (n_files_train+n_files_validate+n_files_test)]\n",
    "    return file_list_train, file_list_validate, file_list_test\n",
    "\n",
    "def file_merger(files): \n",
    "    \"\"\"Given a list of files, open them and merge into one array.  \n",
    "    Inputs:\n",
    "        files | list | list of paths to the .npz files\n",
    "    Returns\n",
    "        X | r4 array | data\n",
    "        Y_class | r2 array | class labels, ? x n_classes\n",
    "        Y_loc | r2 array | locations of signals, ? x 4 (as x,y, width, heigh)\n",
    "    History:\n",
    "        2020/10/?? | MEG | Written\n",
    "        2020/11/11 | MEG | Update to remove various input arguments\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def open_synthetic_data_npz(name_with_path):\n",
    "        \"\"\"Open a file data file \"\"\"  \n",
    "        data = np.load(name_with_path)\n",
    "        X = data['X']\n",
    "        Y_class = data['Y_class']\n",
    "        Y_loc = data['Y_loc']\n",
    "        return X, Y_class, Y_loc\n",
    "\n",
    "    n_files = len(files)\n",
    "    for i, file in enumerate(files):\n",
    "        X_batch, Y_class_batch, Y_loc_batch = open_synthetic_data_npz(file)\n",
    "        if i == 0:\n",
    "            n_data_per_file = X_batch.shape[0]\n",
    "            # initate array, rank4 for image, get the size from the first file\n",
    "            X = np.zeros((n_data_per_file * n_files, X_batch.shape[1], X_batch.shape[2], X_batch.shape[3]))\n",
    "            # should be flexible with class labels or one hot encoding\n",
    "            Y_class = np.zeros((n_data_per_file  * n_files, Y_class_batch.shape[1]))              \n",
    "            Y_loc = np.zeros((n_data_per_file * n_files, 4))    # four columns for bounding box\n",
    "            \n",
    "        \n",
    "        X[i*n_data_per_file:(i*n_data_per_file)+n_data_per_file,:,:,:] = X_batch\n",
    "        Y_class[i*n_data_per_file:(i*n_data_per_file)+n_data_per_file,:] = Y_class_batch\n",
    "        Y_loc[i*n_data_per_file:(i*n_data_per_file)+n_data_per_file,:] = Y_loc_batch\n",
    "    \n",
    "    return X, Y_class, Y_loc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70293e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain the fully connected part of the network)\n",
    "cnn_settings = {'input_range'       : {'min':0, 'max':255}}\n",
    "# the number of files that will be used to train the network\n",
    "cnn_settings['n_files_train']     = 22      \n",
    "# the number of files that wil be used to validate the network (i.e. passed through once per epoch)\n",
    "cnn_settings['n_files_validate']  = 2       \n",
    "# the number of files held back for testing. \n",
    "cnn_settings['n_files_test']      = 2       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc339512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_two_head_model(model_input, n_class_outputs = 3):\n",
    "    \"\"\" Define the two headed model that we have designed to performed classification and localisation.  \n",
    "    Inputs:\n",
    "        model_input | tensorflow.python.framework.ops.Tensor | \n",
    "                      The shape of the tensor that will be input to our model. \n",
    "                      Usually the output of VGG16 (?x7x7x512)  Nb ? = batch size.  \n",
    "        n_class_output | int | For a one hot encoding style output, there must be as many neurons as classes\n",
    "    Returns:\n",
    "        output_class |tensorflow.python.framework.ops.Tensor | \n",
    "                      The shape of the tensor output by the classifiction head.  Usually ?x3\n",
    "        output_loc | tensorflow.python.framework.ops.Tensor | \n",
    "                     The shape of the tensor output by the localisation head.  Usually ?x4\n",
    "    History:\n",
    "        2020_11_11 | MEG | Written\n",
    "    \"\"\"    \n",
    "    vgg16_block_1to5_flat = Flatten(name = 'vgg16_block_1to5_flat')(model_input)  \n",
    "    # flatten the model input (ie deep representation turned into a column vector)\n",
    "    # 1: the clasification head\n",
    "    x = Dropout(0.2, name='class_dropout1')(vgg16_block_1to5_flat)\n",
    "    # add a fully connected layer\n",
    "    x = Dense(256, activation='relu', name='class_dense1')(x)                                                 \n",
    "    x = Dropout(0.2, name='class_dropout2')(x)\n",
    "    # add a fully connected layer\n",
    "    x = Dense(128, activation='relu', name='class_dense2')(x)                                                \n",
    "    # and an ouput layer with 7 outputs (ie one per label)\n",
    "    output_class = Dense(n_class_outputs, activation='softmax',  name = 'class_dense3')(x)                 \n",
    "    \n",
    "    \n",
    "    # 2: the localization head\n",
    "    x = Dense(2048, activation='relu', name='loc_dense1')(vgg16_block_1to5_flat) \n",
    "    # add a fully connected layer\n",
    "    x = Dense(1024, activation='relu', name='loc_dense2')(x)                                                \n",
    "    # add a fully connected layer\n",
    "    x = Dense(1024, activation='relu', name='loc_dense3')(x)                                                \n",
    "    x = Dropout(0.2, name='loc_dropout1')(x)\n",
    "    # add a fully connected layer\n",
    "    x = Dense(512, activation='relu', name='loc_dense4')(x)                                                 \n",
    "    # add a fully connected layer\n",
    "    x = Dense(128, activation='relu', name='loc_dense5')(x)                                                 \n",
    "    output_loc = Dense(4, name='loc_dense6')(x)        \n",
    "    \n",
    "    return output_class, output_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0296e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, compile, and train the model\n",
    "# VGG16 is used for its convolutional layers and weights (but no fully connected part as we define out own )\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))       \n",
    "# the input to the fully connected model must be the same shape as the output of the 5th block of vgg16\n",
    "fc_model_input = Input(shape = vgg16_block_1to5.output_shape[1:])               \n",
    "# build the full connected part of the model, and get the two model outputs\n",
    "output_class, output_loc = define_two_head_model(fc_model_input, len(synthetic_ifgs_settings['defo_sources']))  \n",
    "# define the model.  Input is the shape of vgg16 block 1 to 5 output, and there are two outputs (hence list)\n",
    "vgg16_2head_fc = Model(inputs=fc_model_input, outputs=[output_class, output_loc])                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab4b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_double_network(model, files, n_epochs, loss_names,\n",
    "                                    X_validate, Y_class_validate, Y_loc_validate, n_classes):\n",
    "    \"\"\"Train a double headed model using training data stored in separate files.  \n",
    "    Inputs:\n",
    "        model | keras model | the model to be trained\n",
    "        files | list | list of paths and filenames for the files used during training\n",
    "        n_epochs | int | number of epochs to train for\n",
    "        loss names | list | names of outputs of losses (e.g. \"class_dense3_loss)\n",
    "    Returns\n",
    "        model | keras model | updated by the fit process\n",
    "        metrics_loss | r2 array | columns are: total loss/class loss/loc loss /validate total loss/validate \n",
    "                                               class loss/ validate loc loss\n",
    "        metrics_class | r2 array | columns are class accuracy, validation class accuracy\n",
    "        \n",
    "    2019/03/25 | Written.  \n",
    "    \"\"\"  \n",
    "    n_files_train = len(files)          # get the number of training files\n",
    "    \n",
    "    metrics_class = np.zeros((n_files_train*n_epochs, 2))   # train class acuracy, valiate class accuracy\n",
    "    # total loss/class loss/loc loss /validate total loss/validate class loss/ validate loc loss\n",
    "    metrics_loss = np.zeros((n_files_train*n_epochs, 6))     \n",
    "    for e in range(n_epochs):     # loop through the number of epochs\n",
    "        for file_num, file in enumerate(files):    # for each epoch, loop through all files once\n",
    "        \n",
    "            data = np.load(file)\n",
    "            X_batch = data['X']\n",
    "            Y_batch_class = data['Y_class']\n",
    "            Y_batch_loc = data['Y_loc']\n",
    "            \n",
    "            if n_classes !=  Y_batch_class.shape[1]:\n",
    "                # convert to one hot encoding (from class labels)\n",
    "                Y_batch_class = keras.utils.to_categorical(Y_batch_class, n_classes, dtype='float32')                     \n",
    "\n",
    "            history_train_temp = model.fit(X_batch, [Y_batch_class, Y_batch_loc], batch_size=32,\n",
    "                                           epochs=1, verbose = 0)\n",
    "            # main loss   \n",
    "            metrics_loss[(e*n_files_train)+file_num, 0] = history_train_temp.history['loss'][0]   \n",
    "            # class loss\n",
    "            metrics_loss[(e*n_files_train)+file_num, 1] = history_train_temp.history[loss_names[0]][0]\n",
    "            # localization loss\n",
    "            metrics_loss[(e*n_files_train)+file_num, 2] = history_train_temp.history[loss_names[1]][0]   \n",
    "            metrics_class[(e*n_files_train)+file_num, 0] = history_train_temp.history['class_dense3_accuracy'][0]           # classification accuracy        \n",
    "            print(f'Epoch {e}, file {file_num}: Loss = {round(metrics_loss[(e*n_files_train)+file_num, 0],0)}, '\n",
    "                                       f'Class. loss = {round(metrics_loss[(e*n_files_train)+file_num, 1],2)}, '\n",
    "                                       f'Class. acc. = {round(metrics_class[(e*n_files_train)+file_num, 0],2)}, '\n",
    "                                         f'Loc. loss = {round(metrics_loss[(e*n_files_train)+file_num, 2],0)}')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        history_validate_temp = model.evaluate(X_validate, [Y_class_validate, Y_loc_validate], \n",
    "                                               batch_size = 32, verbose = 0)\n",
    "        metrics_loss[(e*n_files_train)+file_num, 3] = history_validate_temp[0]     # main loss\n",
    "        metrics_loss[(e*n_files_train)+file_num, 4] = history_validate_temp[1]     # class loss\n",
    "        metrics_loss[(e*n_files_train)+file_num, 5] = history_validate_temp[2]     # localisation loss\n",
    "        metrics_class[(e*n_files_train)+file_num, 1] = history_validate_temp[3]    # classification  accuracy\n",
    "        print(f'Epoch {e}, valid.: Loss = {round(metrics_loss[(e*n_files_train)+file_num, 3],0)}, '\n",
    "                          f'Class. loss = {round(metrics_loss[(e*n_files_train)+file_num, 4],2)}, '\n",
    "                          f'Class. acc. = {round(metrics_class[(e*n_files_train)+file_num, 1],2)}, '\n",
    "                            f'Loc. loss = {round(metrics_loss[(e*n_files_train)+file_num, 5],0)}')\n",
    "    \n",
    "    # class loss, validate class loss, class accuracy, validate class accuracy\n",
    "    metrics_class = np.hstack((metrics_loss[:,1:2], metrics_loss[:,4:5], metrics_class ))     \n",
    "    # localisation loss, validate localisation loss, localisation accuracy, validate localisation accuracy\n",
    "    metrics_localisation = np.hstack((metrics_loss[:,2:3], metrics_loss[:,5:]))                        \n",
    "    # class accuracy, validate class accuracy\n",
    "    metrics_combined_loss = np.hstack((metrics_loss[:,1:2], metrics_loss[:,3:4]))         \n",
    "    return model, metrics_class, metrics_localisation, metrics_combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5616f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relative weighting of the two losses (classificaiton and localisation) \n",
    "# to contribute to the global loss.  Classification first, localisation second. \n",
    "fc_loss_weights = [0.05, 0.95]              \n",
    "# the number of epochs to train the fully connected network for \n",
    "# (ie. the number of times all the training data are passed through the model)\n",
    "n_epochs_fc = 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3976c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_training_history(metrics, n_epochs, title = None):\n",
    "    \"\"\"Plot training line graphs for loss and accuracy.  Loss on the left, accuracy on the right.  \n",
    "    Inputs\n",
    "        metrics | r2 array | (n_files * n_epochs) x 2 or 4 matrix,  train loss|validate loss | \n",
    "        train accuracy|validate accuracy. \n",
    "        If no accuracy, only 2 columns\n",
    "        n_epochs | int | number of epochs model was trained for\n",
    "        title | string | title\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    if metrics.shape[1] == 4:        # detemrine if we have accuracy as well as loss\n",
    "        accuracy_flag = True\n",
    "    else:\n",
    "        accuracy_flag = False\n",
    "        \n",
    "    \n",
    "    n_files = metrics.shape[0] / n_epochs\n",
    "    # Figure output\n",
    "    fig1, axes = plt.subplots(1,2)\n",
    "    fig1.canvas.set_window_title(title)\n",
    "    fig1.suptitle(title)\n",
    "    xvals = np.arange(0,metrics.shape[0])\n",
    "    # fewer validation data; find which ones to plot\n",
    "    validation_plot = np.ravel(np.argwhere(metrics[:,1] > 1e-10))                       \n",
    "    axes[0].plot(xvals, metrics[:,0], c = 'k')                                       # training loss\n",
    "    axes[0].plot(xvals[validation_plot], metrics[validation_plot,1], c = 'r')        # validation loss\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend(['train', 'validate'], loc='upper left')\n",
    "    axes[0].axhline(y=0, color='k', alpha=0.5)\n",
    "    \n",
    "    if accuracy_flag:\n",
    "        axes[1].plot(xvals, metrics[:,2], c = 'k')                                       # training accuracy\n",
    "        axes[1].plot(xvals[validation_plot], metrics[validation_plot,3], c = 'r')        # validation accuracy\n",
    "        axes[1].set_ylim([0,1])\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].yaxis.tick_right()\n",
    "        axes[1].legend(['train', 'validate'], loc='upper right')\n",
    "        \n",
    "    #\n",
    "    titles = ['Training loss', 'Training accuracy']\n",
    "    for i in range(2):\n",
    "        axes[i].set_title(titles[i])\n",
    "        # change so a tick only after each epoch (and not each file)\n",
    "        axes[i].set_xticks(np.arange(0,metrics.shape[0],2*n_files))     \n",
    "        axes[i].set_xticklabels(np.arange(0,n_epochs, 2))   # number ticks\n",
    "        axes[i].set_xlabel('Epoch number')\n",
    "\n",
    "    if not accuracy_flag:\n",
    "        axes[1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ba6b5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune the 5th block and the fully connected part of the network):\n",
    "\n",
    "# as per fc_loss_weights, but by changing these more emphasis can be placed on either the clasification \n",
    "# or localisation loss.  \n",
    "block5_loss_weights = [0.05, 0.95]  \n",
    "\n",
    "# We have to set a learning rate manually as an adaptive approach (e.g. NADAM) will be high initially,\n",
    "# and therefore make large updates that will wreck the model (as we're just fine-tuning a model so have \n",
    "# something good to start with)\n",
    "block5_lr = 1.5e-8 \n",
    "\n",
    "# the number of epochs to fine-tune for \n",
    "# (ie. the number of times all the training data are passed through the model)\n",
    "n_epochs_block5 = 10                \n",
    "   \n",
    "np.random.seed(0)                   # 0 used in the example\n",
    "              \n",
    "#%% Fine-tune the 5th convolutional block and the fully connected network.  \n",
    "\n",
    "# VGG16 is used for its convolutional layers and weights (but no fully connected part as we define out own )\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))  \n",
    "# build the fully connected part of the model, and get the two model outputs\n",
    "output_class, output_loc = define_two_head_model(vgg16_block_1to5.output,\n",
    "                                                 len(synthetic_ifgs_settings['defo_sources']))        \n",
    "\n",
    "\n",
    "vgg16_2head = Model(inputs=vgg16_block_1to5.input, outputs=[output_class, output_loc])                                           # define the full model\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b9351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c6847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190561f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
